{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42a2e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, None, 64)             576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 64)             640       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, None, 128),          98816     ['embedding[0][0]']           \n",
      "                              (None, 128),                                                        \n",
      "                              (None, 128)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 128),          98816     ['embedding_1[0][0]',         \n",
      "                              (None, 128),                           'lstm[0][1]',                \n",
      "                              (None, 128)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, None, 128)            0         ['lstm_1[0][0]',              \n",
      "                                                                     'lstm[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, 256)            0         ['lstm_1[0][0]',              \n",
      "                                                                     'attention[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 10)             2570      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 201418 (786.79 KB)\n",
      "Trainable params: 201418 (786.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 21:50:39.835021: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x357d1a940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder-Decoder with Attention: Educational Notebook\n",
    "\n",
    "# üìò Introduction\n",
    "# In this notebook, we implement a simple encoder-decoder model with attention\n",
    "# to understand how sequence-to-sequence models work in NLP.\n",
    "\n",
    "# üõ†Ô∏è Requirements\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# üßæ Sample Data (English to French)\n",
    "english_sentences = [\"i love you\", \"he is smart\", \"she is nice\"]\n",
    "french_sentences = [\"je t'aime\", \"il est intelligent\", \"elle est gentille\"]\n",
    "\n",
    "# üî† Tokenization\n",
    "input_tokenizer = Tokenizer()\n",
    "target_tokenizer = Tokenizer()\n",
    "\n",
    "input_tokenizer.fit_on_texts(english_sentences)\n",
    "target_tokenizer.fit_on_texts([\"<start> \"+s+\" <end>\" for s in french_sentences])\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(english_sentences)\n",
    "target_sequences = target_tokenizer.texts_to_sequences([\"<start> \"+s+\" <end>\" for s in french_sentences])\n",
    "\n",
    "max_input_len = max([len(seq) for seq in input_sequences])\n",
    "max_target_len = max([len(seq) for seq in target_sequences])\n",
    "\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
    "decoder_input_data = pad_sequences([seq[:-1] for seq in target_sequences], maxlen=max_target_len-1, padding='post')\n",
    "decoder_target_data = pad_sequences([seq[1:] for seq in target_sequences], maxlen=max_target_len-1, padding='post')\n",
    "\n",
    "# üß± Parameters\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "embedding_dim = 64\n",
    "lstm_units = 128\n",
    "\n",
    "# üîß Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = tf.keras.layers.Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True, return_sequences=True)(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# üîß Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = tf.keras.layers.Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm_output, _, _ = LSTM(lstm_units, return_sequences=True, return_state=True)(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# üîç Attention\n",
    "attention_layer = Attention()\n",
    "context_vector = attention_layer([decoder_lstm_output, encoder_lstm])\n",
    "concat = tf.keras.layers.Concatenate(axis=-1)([decoder_lstm_output, context_vector])\n",
    "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(concat)\n",
    "\n",
    "# ‚úÖ Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# üß™ Dummy Training (for illustration only)\n",
    "y = np.expand_dims(decoder_target_data, -1)\n",
    "model.fit([encoder_input_data, decoder_input_data], y, epochs=100, verbose=0)\n",
    "\n",
    "# üìå Notes:\n",
    "# - This model learns to translate short English sentences to French using attention.\n",
    "# - Attention allows the decoder to focus on relevant parts of the input sequence.\n",
    "# - This notebook is meant for educational purposes and is not production-grade.\n",
    "\n",
    "# üß† Next: You can expand this to use real datasets like many-to-many translation tasks using datasets like Multi30k or WMT.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
